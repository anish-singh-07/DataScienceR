---
#title: "Models & Results"
output: 
  html_document:
    css: "../../www/custom.css"
---


```{r setup, include = FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
```

<style>
body {
text-align: justify}
</style>

<div class="bodyRmdContent">
## 8) Modelling and Results
***
<p style="text-align: justify;">
We have used 4 machine learning models and compared different evaluation metrics for them to achieve classification.Combination of all the features which was generated by the feature extraction phase was used as input for the models.<br>

Following features are used in Machine Learning Models ( except baseline model):-
<li>Sentiments - 9 dimensions using Syuzhet</li>
<li>Word2Vec - 25 dimensions</li>
<li>Glove embeddings -  25 dimensions using pertained model. </li>

We faced a challenge with our dataset as the target is labelled author level for all his tweets combined altogether. As the author target is labelled as 1(which means the author spreads hate), using this information we cannot naively assume that all the tweets belonging to him are hate spreader tweets. Therefore, we have extracted the target called “CustomTarget” at tweet level. <br>
The “CustomTarget” is extracted as mentioned below. Syuzhet package gives a positive, negative score given a text or sentence. Based on these scores, “CustomTarget” is decided.<br> 

Algorithm for extracting the “CustomTarget”
```{r, eval=FALSE}
if(positive == 0 && negative ==0)
{
  #ignore the tweet
}
else if(negative >= positive)
{
   Label the CustomTarget to 1
}
else 
{
   Label the CustomTarget as 0
}
```

<p style="text-align: justify;">

In the above algorithm we have made 2 assumptions.
<li>Ignoring the tweet if the positive and negative score is 0 as the text could be neutral.</li>
<li> We are considering the tweet as negative even when the positive and negative scores are equal. We are biased towards making it as negative.</li>
**Class Data Distribution**<br>
<p style="text-align: justify;">
For training and test data we created a class distribution of data and observed that for training around 55% of data is related to authors belonging to class 0(Not spreading hate) and 45% to class 1(spreading hate).With this dataset we started with model training.
</p>
```{r,eval=FALSE}
barplot(prop.table(table(data_train$Custom_Target)),
        col = "#219ebc",
        ylim = c(0,1),
        main = "Training Class Distribution")
```{r,rout.width='80%', fig.align='center', echo = FALSE}
knitr::include_graphics(here("plots","train_dist.png"))
```
<p style="text-align: justify;">
Similarly for test data around 55% of data is related to authors belonging to class 0(Not spreading hate) and 45% to class 1(spreading hate).
</p>
```{r,rout.width='80%', fig.align='center', echo = FALSE}
knitr::include_graphics(here("plots","test_dist.png"))
```





**Evaluation**<br>
For evaluation we have used Confusion matrix to evaluate the model’s performance. The measures recorded are Accuracy, Precision, Recall and F1-Score.<br>
We have compared the performance of all the models to baseline model of TF-IDF.
<br>
```{r,out.width='80%', fig.align='center', echo = FALSE}
knitr::include_graphics(here("plots","Per_author_result.png"))
```
**Discussion**<br>
<p style="text-align: justify;">
<li>We are able to achieve 61% Accuracy with TF-IDF matrix using Naive Byes model. We are using this model as our baseline for comparison to other features and models.</li>
<li>Other feature set include Sentiment Analysis matrix , Word2Vec and Glove embeddings of 25 dimensions per tweet.</li>
<li>Feature Set of all tweets is combined by per Author and fed to various Machine learning models. Highest reported accuracy is 63% by Naive Bayes classifier.</li>
</p>


<p style="text-align: justify;">
**Adaptations made to Predicted Test Values for getting the Author Class**

All the models except the baseline model will be trained on the extracted feature set (Sentiments, Word2Vec and Glove) with “CustomTarget” as target. Once the models are trained then we predict the test data.
Predictions are generated on the test data at tweet level as the models are trained for each individual tweet instead author level. Therefore in order to get the author level predictions we applied the following algorithm.<br>

Algorithm to predict labels for Authors 
```{r, eval=FALSE}
while(True)
{
  CountOnes = 0
  CountZeros = 0
	while(Author_ID_IsSame)
	{
	   if(predicted_Target == 1)
	   {
	      CountOnes  +=1
     } 
     else
     {
        CountZeros  +=1
     }
	}
	
  if(CountOnes >= CountZeros)
  {
     Author_Predicted_Target = 1
  } 
  else
  {
     Author_Predicted_Target = 1
  }
	
  if(No_Data)
  {
	  break
  }
}

```




We used the predictions produced by above algorithm for evaluating the model on Test dataset.
```{r,out.width='80%', fig.align='center', echo = FALSE}
knitr::include_graphics(here("plots","per_tweet_result.png"))
```
**Discussion**<br>

<li>Using Extracted Features, highest Accuracy of 86% is achieved by Logistic Regression followed by Decision tree with 85%.</li> 
<li>We got better results with Word2Vec alone, therefore we decided to drop Glove embeddings for further analysis. This will help us to reduce computation time during training and testing.</li>

</div>

